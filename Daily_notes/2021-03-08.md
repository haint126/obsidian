# What have been done yesterday

- [[Daily_notes/2021-03-06]]

# What to do to day
![[99_Others/Quotes/quote_20190101#^different-perspectives]]

-   Code LSTM and Transformer
-   Explain why Transformer is faster than LSTM

# What to read

- [ ] [# Compositional Explanations for Image Classifiers](https://arxiv.org/abs/2103.03622)
- [ ] [# Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth](https://arxiv.org/abs/2103.03404)
- [ ] [Multimodal Neurons in Artificial Neural Networks](https://openai.com/blog/multimodal-neurons/)
- [ ] [Self-supervised learning: The dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence)
- [ ] [# Self-supervised Pretraining of Visual Features in the Wild](https://arxiv.org/abs/2103.01988)
- [ ] [# Do Transformer Modifications Transfer Across Implementations and Applications?](https://arxiv.org/abs/2102.11972)
- [ ] [# Ultra-Data-Efficient GAN Training: Drawing A Lottery Ticket First, Then Training It Toughly](https://arxiv.org/abs/2103.00397)



---
Status: #reading

Tags: 

References:

Related:
