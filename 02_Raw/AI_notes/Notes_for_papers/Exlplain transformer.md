#



# 

---
Status: #not_done 

Tags: #transformer #explain #computer_vision 

References:
- [ ] [# Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://arxiv.org/abs/2103.15679)
- [ ] [Explaining multi-headed self attention](https://theaisummer.com/self-attention/)





Related:
