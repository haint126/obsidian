# Bagging
- Bagging can reduce the variance within a learning algorithm.
- We often use it with low-bias models, like unpruned decision trees
- This is particularly helpful with high-dimensional data, where missing values can lead to higher variance, making it more prone to overfitting and preventing accurate generalization to new datasets.

- Bagging trains a group of models in parallel and independently from each other.
- Each model uses a subset of the data randomly selected with replacement from the original dataset. 
	- That's why bagging is also known as "bootstrap aggregating:" because it draws bootstrap samples from the training dataset.

# 

---
- Status: #done

- Tags: #basic_ai_knowledge 

- References:
	- [Reference](https://www.ibm.com/cloud/learn/bagging)

- Related:
	- 
