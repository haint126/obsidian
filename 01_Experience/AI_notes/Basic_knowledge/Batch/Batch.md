# Gradient Descent
- With Gradient Descent, we take samples from the training dataset, run them through the model, and determine how far our results are from the ones we expect.

- We then use this "error" to compute how much we need to update the model weights to improve the results.

- A critical decision we need to make is how many samples we use on every iteration.

- We have three choices to choose : 
	- Use a single sample of data.
	- Use all of the data at once.
	- Use some of the data.


# Stochastic Gradient Descent
- Stochastic Gradient Descent (SGD) is using **a single sample** of data on every iteration


# Batch Gradient Descent
- Batch Gradient Descent is using **all of the data** at once.


# Mini-Batch Gradient Descent
- Mini-Batch Gradient Descent is using **some of the data** (more than one sample but fewer than the entire dataset)

The algorithm works like Batch Gradient Descent, with the only difference that we use fewer samples




# 

---
- Status: #done

- Tags: #machine_learning  #basic_ai_knowledge 

- References:
	- 

- Related:
	- 