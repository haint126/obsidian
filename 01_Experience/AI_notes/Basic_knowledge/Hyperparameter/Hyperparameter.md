# 

The goal of hyperparameter tuning is to choose the optimal parameters for a learning algorithm to train a model.

We call "hyperparameters" to those parameters and settings that we can use to control the learning process.

In contrast, we use "parameters" to refer to variables internal to a model and whose value can be estimated from data.

A good way of thinking about it:

• Parameters: We don't control them manually. We learn their values during training.

• Hyperparameters: We do control them manually. They act like configuration settings for our models.


Regarding the name, the prefix "hyper" means "over; beyond; above." 

Hence "hyperparameters" end up meaning something like "The parameters that are above the other parameters."


# 
	 
---
- Status: #done 

- Tags: #machine_learning #basic_ai_knowledge 

- References:
	- 

- Related:
	- 