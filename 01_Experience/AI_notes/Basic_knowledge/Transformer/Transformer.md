# Transformer vs LSTM
- 
	- [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)Â models are sequential and process elements in order
	- Transformers process sequences as a whole rather than element by element
		- => This allows us to parallelize Transformers, which enables us to process much more data when compared with an LSTM.

- Both can understand the relationship of two elements next to each other in a sequence
- The power of the Transformer architecture is understanding the relationship between elements that are far from each other.



# 

---
- Status: #done

- Tags: #basic_ai_knowledge #LSTM #transformer 

- References:
	- 

- Related:
	- 
