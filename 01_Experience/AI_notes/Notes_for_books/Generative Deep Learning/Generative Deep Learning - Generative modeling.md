# What Is Generative Modeling?

A generative model describes how a dataset is generated, in terms of a probabilistic model. By sampling from this model, we are able to generate new data


A summary of a typical generative modeling process is shown below:

![[Images/AI_note_images/GAN_generative_model.png]]

In the dataset, each data point in the dataset is called an **observation** which consists of many features.

For an image generation problem, the features are usually the individual pixel values. It is our goal to build a model that can generate new sets of features that look as if they have been created using the same rules as the original data.

A generative model must also be **probabilistic** rather than deterministic so that the model produces different output every time. Therefore, the model must include a stochastic (random) element that influences the individual samples generated by the model.

In other words, we can imagine that there is some unknown probabilistic distribution that explains why some images are likely to be found in the training dataset and other images are not. It is our job to:
- Build a model that mimics this distribution as closely as possible
- Then sample from it to generate new, distinct observations that look as if they could have been included in the original training set.

# Discriminative model

A discriminative model differentiates between groups of observations with the same label. For this reason, discriminative modeling is synonymous with supervised learning, or learning a function that maps an input to an output using a labeled dataset.

A summary of a typical generative modeling process is shown below:

![[Images/AI_note_images/GAN_discriminative_model.png]]

# Generative Versus Discriminative Modeling

>Discriminative modeling estimates $p(x|y)$ - the probability of a label *y* given observation *x*.

>Generative modeling estimates $p(x)$ - the probability of observing observation *x*. If the dataset is labeled, we can also build a generative model that  estimates the distribution $p(y|x)$.

In other words, discriminative modeling attempts to estimate the probability that an observation x belongs to category y. Meanwhile, generative modeling  attempts to estimate the probability of seeing the observation at all (create results that have a high chance of belonging to the original training dataset.


# The Generative Modeling Framework

### The Generative Modeling Framework
- We have a dataset of observations $X$.
- We assume that the observations have been generated according to some unknown distribution, $P_{data}$.
- A generative model $P_{model}$ tries to mimic $P_{data}$ . If we achieve this goal, we can sample from $P_{model}$ to generate observations that appear to have been drawn from $P_{data}$ .
- We are impressed by $P_{model}$ if:
	- Rule 1: It can generate examples that appear to have been drawn from $P_{data}$ .
	- Rule 2: It can generate examples that are suitably different from the observations in $X$. In other words, the model shouldn't simply reproduce things it has already seen.

Here is how to determine whether a generated example is good or bad:
$P_{data}$ = the world's land
![[Images/AI_note_images/GAN_good_generated_example.png]]

# Probabilistic Generative Models

![[Images/AI_note_images/GAN_probabilistic_generative_model.png]]

### Sample space
The sample space is the complete set of all values an observation x can take.

### Probability density function
A probability density function (or simply density function), $p(x)$, is a function that maps a point x in the sample space to a number between 0 and 1. 
$$p(x) \in [0, 1]$$
The sum of the density function over all points in the sample space must equal 1, so that it is a well-defined probability distribution.
$$\sum_{x \in X} p_\theta (x) = 1$$

### Parametric modeling
A parametric model, $p_\theta (x)$, is a family of density **functions** that can be described using a finite number of parameters, $\theta$.

The family of all possible boxes you could draw on [[Images/AI_note_images/GAN_bad_likelihood.png]] is an example of a parametric model. In this case, there are four parameters: the coordinates of the bottom-left $(\theta _1, \theta _2)$ and top-right $(\theta _3, \theta _4)$ corners of the box.

Thus, each density function $p_\theta (x)$ in this parametric model (i.e., each box) can be uniquely represented by four numbers, $\theta  = (\theta _1, \theta _2, \theta _3, \theta _4)$ .

### Likelihood
- The likelihood $L(\theta |x)$ of a parameter set $\theta$ is a function that measures the plausibility of $\theta$, given some observed point $x$.
$$L(\theta |x) = p_\theta (x)$$
- If we have a whole dataset X of **independent** observations then we can write:
$$L(\theta |X) = \prod_{x \in X} p_\theta (x)$$

- To reduce computation, we often use the log-likelihood l instead:
$$l(\theta |X) = \sum_{x \in X} log\, {p_\theta (x)}$$

![[Images/AI_note_images/GAN_bad_likelihood.png]]
### Maximum likelihood estimation
Maximum likelihood estimation is the technique that allows us to estimate $\hat{\theta }$ - the set of parameters $\theta$ of a density function, $p_\theta (x)$ , that are most likely to explain some observed data $X$.

$$\hat{\theta } = \underset{\theta }{arg\,max} \: L(\theta |X)$$

$\hat{\theta }$ is also called the maximum likelihood estimate (MLE).

# Naive Bayes

**Condition to use: Individual features are independent**

Naive Bayes for all features $x_j, x_k$:
$$p(x_j|x_k) = p(x_j)$$

For above example, we first make use of the chain rule of probability to write the density function as a product of conditional probabilities:
$${
\begin{align}
p(x) &= p(x_1, \ldots, x_K)\\
&= \prod_{k=1}^{K} p(x_k|x_1, \ldots, x_{k-1})
\end{align}
}$$

Then apply Naive Bayes assumption:
$$p(x) = \prod_{k=1}^{K} p(x_k)$$

*Verify?* For Naive Bayes to work well, the sample space need to be small.

# The Challenges of Generative Modeling

- How does the model cope with the high degree of conditional dependence between features?
- How does the model find one of the tiny proportion of satisfying possible generated observations among a high-dimensional sample space?

=> We need a model that can infer relevant structure from the data, rather than being told which assumptions to make in advance.

# Representation learning






---
Status: #postpone 

Tags: 

References:

Related: [[01_Experience/AI_notes/Notes_for_books/Generative Deep Learning/Generative Deep Learning]]
