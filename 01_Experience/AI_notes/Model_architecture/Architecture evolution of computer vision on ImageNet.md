# Architecture evolution of computer vision on ImageNet
- The works that perhaps receive the most attention are novel architectures. They are:
	- [AlexNet (Krizhevsky et al., 2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
	- [VGG (Simonyan & Zisserman, 2014)](https://arxiv.org/abs/1409.1556v6)
	- [ResNet (He et al., 2015)](https://arxiv.org/abs/1512.03385v1)
	- [Inception (Szegedy et al., 2015; 2016) (here is link for Inveptionv4)](https://arxiv.org/abs/1602.07261v2)
	- [ResNeXt (Xie et al.,2017)](https://arxiv.org/abs/1611.05431v2)
- Automated search strategies for designing architectures have further pushed the state-of-the-art, notably with:
	- [NasNet-A (Zoph et al., 2018)](https://arxiv.org/abs/1707.07012)
	- [AmoebaNet-A (Real et al., 2019)](https://arxiv.org/abs/1802.01548v7)
	- [EfficientNet (Tan & Le, 2019)](https://arxiv.org/abs/1905.11946v5)
- There have also been efforts in going beyond standard ConvNets for image classification, by:
	- Adapting self-attention ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))
	- Visual domain:
		- [Bello et al., 2019](https://arxiv.org/abs/1904.09925)
		- [Ramachandran et al., 2019](https://arxiv.org/abs/1906.05909)
		- [Hu et al., 2019](https://arxiv.org/abs/1904.11491)
		- [Shen et al., 2020](https://arxiv.org/abs/2010.03019)
		- [**Dosovitskiy et al., 2020**](https://arxiv.org/abs/2010.11929)
	- Using alternatives such as lambda layers ([Bello, 2021](https://arxiv.org/abs/2102.08602))


# 

---
Status: #done 

Tags: #computer_vision #basic_knowledge #attention 

References:
-  [# Revisiting ResNets: Improved Training and Scaling Strategies](https://arxiv.org/abs/2103.07579)

Related:
